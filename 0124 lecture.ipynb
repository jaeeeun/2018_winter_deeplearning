{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 5강 \n",
    "# learning rate에 따른 정확성 확인\n",
    "# learning_rate와 학습 횟수를 적절하게 조절해야 최대한의 학습속도를 낼 수 있다\n",
    "# 최소한의 학습횟수로 최대한의 효과를 내야 효율적이다\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "LEARNING_RATE=0.01\n",
    "\n",
    "x_data = [[ 1, 2, 1], [ 1, 3, 2], [ 1, 3, 4], [ 1, 5, 5], [ 1, 7, 5], [ 1, 2, 5], [ 1, 6, 6], [ 1, 7, 7]] \n",
    "y_data = [[ 0, 0, 1], [ 0, 0, 1], [ 0, 0, 1], [ 0, 1, 0], [ 0, 1, 0], [ 0, 1, 0], [ 1, 0, 0], [ 1, 0, 0]] \n",
    "\n",
    "# Evaluation our model using this test dataset\n",
    "\n",
    "x_test = [[ 2, 1, 1], [ 3, 1, 2], [ 3, 3, 4]]\n",
    "y_test = [[ 0, 0, 1], [ 0, 0, 1], [ 0, 0, 1]]\n",
    "\n",
    "\n",
    "X= tf.placeholder (\"float\", [None, 3])\n",
    "Y= tf.placeholder (\"float\", [ None, 3]) \n",
    "W= tf.Variable (tf.random_normal ([ 3, 3]))\n",
    "b= tf.Variable (tf.random_normal ([ 3]))\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp (logits ) / reduce_sum (exp (logits ), dim )\n",
    "hypothesis = tf.nn.softmax (tf.matmul (X, W) + b)\n",
    "\n",
    "# Cross entropy cost /loss \n",
    "cost = tf.reduce_mean (-tf.reduce_sum (Y* tf.log (hypothesis ), axis =1))\n",
    "\n",
    "# Try to change learning_rate to small numbers \n",
    "optimizer = tf.train.GradientDescentOptimizer (learning_rate =LEARNING_RATE). minimize (cost )\n",
    "\n",
    "\n",
    "# Correct prediction Test model \n",
    "prediction = tf.arg_max (hypothesis , 1)\n",
    "is_correct = tf.equal (prediction , tf.arg_max (Y, 1))\n",
    "accuracy = tf.reduce_mean (tf.cast (is_correct , tf.float32)) \n",
    "\n",
    "\n",
    "\n",
    "# Launch graph with \n",
    "# tf.Session으로 생성된 sess라는 변수를 통해서 쓴다\n",
    "# sess=tf.Session()이랑 똑같은데, 이 블럭에서만 쓴다\n",
    "\n",
    "with tf.Session () as sess :\n",
    "\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run (tf.global_variables_initializer ()) \n",
    "    for step in range (101 ): \n",
    "        cost_val , W_val , _ = sess.run ([cost , W, optimizer ], feed_dict ={ X: x_data , Y: y_data })\n",
    "        if step%50 == 0 :\n",
    "            print (\"\\nstep : \", step , \"\\ncost_val : \", cost_val , \"\\nW_val :\\n \", W_val )\n",
    "    \n",
    "    # predict\n",
    "    print (\"\\nPrediction :\" , sess.run (prediction , feed_dict ={ X: x_test })) \n",
    "\n",
    "    # Calculate the accuracy \n",
    "    print (\"\\nAccuracy: \" , sess.run (accuracy , feed_dict ={ X: x_test , Y: y_test }))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화 되지 않은 데이터\n",
    "xy = np.array ([[ 828.659973 , 833.450012 , 908100 , 828.349976 , 831.659973 ],\n",
    "                [823.02002 , 828.070007 , 1828100 , 821.655029 , 828.070007 ], \n",
    "                [819.929993 , 824.400024 , 1438100 , 818.97998 , 824.159973 ], \n",
    "                [816 , 820.958984 , 1008100 , 815.48999 , 819.23999 ],\n",
    "                [819.359985 , 823 , 1188100 , 818.469971 , 818.97998 ], \n",
    "                [819 , 823 , 1198100 , 816 , 820.450012 ], \n",
    "                [811.700012 , 815.25 , 1098100 , 809.780029 , 813.669983 ], \n",
    "                [809.51001 , 816.659973 , 1398100 , 804.539978 , 809.559998 ]])\n",
    "\n",
    "\n",
    "x_data = xy [:, 0:-1]\n",
    "y_data = xy [:, [ -1]]\n",
    "\n",
    "\n",
    "# placeholders for atensor that will be always fed .\n",
    "X= tf.placeholder (tf.float32, shape =[None , 4])\n",
    "Y= tf.placeholder (tf.float32, shape =[None , 1])\n",
    "\n",
    "W= tf.Variable (tf.random_normal ([ 4, 1]), name ='weight')\n",
    "b= tf.Variable (tf.random_normal ([ 1]), name ='bias')\n",
    "\n",
    "# Hypothesis \n",
    "hypothesis = tf.matmul (X, W) + b\n",
    "\n",
    "# Simplified cost /loss function \n",
    "cost = tf.reduce_mean (tf.square (hypothesis -Y)) \n",
    "\n",
    "# Minimize \n",
    "optimizer = tf.train.GradientDescentOptimizer (learning_rate =1e-5)\n",
    "train = optimizer.minimize (cost )\n",
    "\n",
    "# Launch the graph in asession .\n",
    "sess = tf.Session () \n",
    "\n",
    "# Initializes global variables in the graph .\n",
    "sess.run (tf.global_variables_initializer ()) \n",
    "\n",
    "for step in range (101 ): \n",
    "    cost_val , hy_val , _ = sess.run ([cost , hypothesis , train ], feed_dict ={ X: x_data , Y: y_data }) \n",
    "    if step%10 ==0 :\n",
    "        print (\"\\nstep:\",step, \"\\nCost : \", cost_val , \"\\nPrediction:\\n\", hy_val )\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "# 분석을 못한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화 되지 않은 데이터\n",
    "# min-max scale을 이용하여 정규화를 시킨다\n",
    "\n",
    "def MinMaxScaler (data ):\n",
    "    numerator = data -np.min (data , 0)\n",
    "    denominator = np.max (data , 0) -np.min (data , 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / ( denominator + 1e-7)\n",
    "    \n",
    "\n",
    "xy = np.array ([[ 828.659973 , 833.450012 , 908100 , 828.349976 , 831.659973 ],\n",
    "                [823.02002 , 828.070007 , 1828100 , 821.655029 , 828.070007 ], \n",
    "                [819.929993 , 824.400024 , 1438100 , 818.97998 , 824.159973 ], \n",
    "                [816 , 820.958984 , 1008100 , 815.48999 , 819.23999 ],\n",
    "                [819.359985 , 823 , 1188100 , 818.469971 , 818.97998 ], \n",
    "                [819 , 823 , 1198100 , 816 , 820.450012 ], \n",
    "                [811.700012 , 815.25 , 1098100 , 809.780029 , 813.669983 ], \n",
    "                [809.51001 , 816.659973 , 1398100 , 804.539978 , 809.559998 ]])\n",
    "\n",
    "\n",
    "xy=MinMaxScaler(xy)\n",
    "print(xy)\n",
    "\n",
    "\n",
    "x_data = xy [:, 0:-1]\n",
    "y_data = xy [:, [ -1]]\n",
    "\n",
    "\n",
    "# placeholders for atensor that will be always fed .\n",
    "X= tf.placeholder (tf.float32, shape =[None , 4])\n",
    "Y= tf.placeholder (tf.float32, shape =[None , 1])\n",
    "\n",
    "W= tf.Variable (tf.random_normal ([ 4, 1]), name ='weight')\n",
    "b= tf.Variable (tf.random_normal ([ 1]), name ='bias')\n",
    "\n",
    "# Hypothesis \n",
    "hypothesis = tf.matmul (X, W) + b\n",
    "\n",
    "# Simplified cost /loss function \n",
    "cost = tf.reduce_mean (tf.square (hypothesis -Y)) \n",
    "\n",
    "# Minimize \n",
    "optimizer = tf.train.GradientDescentOptimizer (learning_rate =1e-5)\n",
    "train = optimizer.minimize (cost )\n",
    "\n",
    "# Launch the graph in asession .\n",
    "sess = tf.Session () \n",
    "\n",
    "# Initializes global variables in the graph .\n",
    "sess.run (tf.global_variables_initializer ()) \n",
    "\n",
    "for step in range (101): \n",
    "    cost_val , hy_val , _ = sess.run ([cost , hypothesis , train ], feed_dict ={ X: x_data , Y: y_data }) \n",
    "    if step%10 ==0 :\n",
    "        print (\"\\nstep:\",step, \"\\nCost : \", cost_val , \"\\nPrediction:\\n\", hy_val )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Label :  [2]\n",
      "Prediction :  [2]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24b08a814e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
